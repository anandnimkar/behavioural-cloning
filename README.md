# Behavioural Cloning
### Udacity Self-Driving Car Nanodegree, Project 3
---

### Overview

The objective of this project was to develop a model to clone human driving behaviour while driving a car around a simulated track. The implementation in this repository uses a Deep Convolutional Neural Network to predict the steering angle for each frame generated by the driving simulator. The model preprocessing steps and architecture presented here were heavily influenced by the following two papers from [Nvidia](https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf) and [Comma.ai](https://arxiv.org/pdf/1608.01230v1.pdf).


---
### Generating the Dataset

The Udacity simulator provides two tracks to experiment with both generating training data and evaluating the autonomous driving predictions of the model. For this model, training data was only recorded on track 1 of the simulator. Track 2 was reserved for evaluating the trained model's ability to generalize to a new track with completely different colours, lane markings, hills, turns, and other features.

Track 1 screenshot:
![Track 1](./media/simulator_track1.png)

Track 2 screenshot:
![Track 2](./media/simulator_track2.png)

Training data is recorded by the driving simulator as a set of 160x320 RGB images, three per frame, from three forward-facing cameras sitting left, center, and right on the vehicle's hood. Each frame (set of three images) is labelled with the vehicle's steering angle, throttle value, and brake value recorded from the human inputs.

The total dataset size used to train and validate the model in this repository was 128,217 images, representing the left, center, and right images captured for each frame in the simulator on track 1 only.

The following histogram illustrates the distribution of steering angles recorded for 